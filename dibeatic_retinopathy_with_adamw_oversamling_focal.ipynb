{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11276736,"sourceType":"datasetVersion","datasetId":7048935}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# CELL 1: MODIFIED (Imports, Setup, Config with AdamW/Focal Loss Params)\n\n# Imports and Setup\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nimport gc\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score\nfrom sklearn.utils import class_weight\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, applications\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nimport tensorflow_addons as tfa # <--- Import tensorflow-addons\nfrom tensorflow_addons.losses import CategoricalFocalCrossentropy # <--- Import Focal Loss\n\nSEED = 42\n# Reproducibility\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n\n# --- Distributed Strategy Setup --- (Should remain the same)\ngpus = tf.config.list_physical_devices('GPU')\nprint(f\"Num GPUs Available: {len(gpus)}\")\n\nstrategy = None\nnum_gpus = len(gpus)\nif num_gpus > 1:\n    print(f\"Using MirroredStrategy for {num_gpus} GPUs.\")\n    strategy = tf.distribute.MirroredStrategy()\nelif num_gpus == 1:\n    print(\"Using single GPU.\")\nelse:\n    print(\"WARNING: No GPU detected. Training will be on CPU.\")\n\n# --- Calculate Global Batch Size --- (Keep previous calculation, should be fine with 30GB VRAM)\nper_replica_batch_size = 32\nif num_gpus == 0: # CPU case\n    global_batch_size = per_replica_batch_size\n    print(f\"Using CPU Batch Size: {global_batch_size}\")\nelse:\n    global_batch_size = per_replica_batch_size * num_gpus\n    print(f\"Using Global Batch Size: {global_batch_size} ({per_replica_batch_size} per replica/GPU)\")\n\n# --- NEW: Configuration with AdamW and Focal Loss Params ---\nIMG_SIZE = 224\nN_CLASSES = 5\nEPOCHS_PHASE1 = 30\nEPOCHS_PHASE2 = 25\nLR_PHASE1 = 1e-4       # Initial LR for AdamW Phase 1\nWD_PHASE1 = 1e-4       # Weight Decay for AdamW Phase 1\nLR_PHASE2 = 1e-5       # Fine-tuning LR for AdamW Phase 2\nWD_PHASE2 = 1e-5       # Weight Decay for AdamW Phase 2 (often reduced)\nFOCAL_ALPHA = 0.25     # Alpha parameter for Focal Loss (common value)\nFOCAL_GAMMA = 2.0      # Gamma parameter for Focal Loss (common value)\nPATIENCE_EARLY_STOPPING = 10\nPATIENCE_REDUCE_LR = 5\nMIN_LR = 1e-7","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T07:01:26.806896Z","iopub.execute_input":"2025-04-07T07:01:26.807240Z","iopub.status.idle":"2025-04-07T07:01:30.938729Z","shell.execute_reply.started":"2025-04-07T07:01:26.807197Z","shell.execute_reply":"2025-04-07T07:01:30.937417Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n\nTensorFlow Addons (TFA) has ended development and introduction of new features.\nTFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\nPlease modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n\nFor more information see: https://github.com/tensorflow/addons/issues/2807 \n\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.13.0 and strictly below 2.16.0 (nightly versions are not supported). \n The versions of TensorFlow you are currently using is 2.17.1 and is not supported. \nSome things might work, some things might not.\nIf you were to encounter a bug, do not file an issue.\nIf you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \nYou can find the compatibility matrix in TensorFlow Addon's readme:\nhttps://github.com/tensorflow/addons\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-dbbcb2e97c38>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfa\u001b[0m \u001b[0;31m# <--- Import tensorflow-addons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCategoricalFocalCrossentropy\u001b[0m \u001b[0;31m# <--- Import Focal Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_addons/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Local project imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_addons/activations/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Additional activation functions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgelu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhardshrink\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhardshrink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlisht\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlisht\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_addons/activations/gelu.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorLike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/types.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# New versions of Keras require importing from `keras.src` when\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# importing internal symbols.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2.5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.src.engine'"],"ename":"ModuleNotFoundError","evalue":"No module named 'keras.src.engine'","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"pip install tensorflow addon","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 1.5 (New Cell): Install tensorflow-addons\n# Run this cell once at the beginning\n!pip install tensorflow-addons -q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 2: MODIFIED (Load Data, Plot Original Dist, Oversample, Plot New Dist)\n# (Keep the version from the previous improvement that includes oversampling)\n\n# Load the CSV file containing your metadata\ndf = pd.read_csv('/kaggle/input/diabetic-retinopathy-dataset-ben-graham-applied/train.csv')\nprint(\"Original Dataset shape:\", df.shape)\nprint(df.head())\n\n# Check for missing values\nprint(\"\\nMissing values per column:\")\nprint(df.isnull().sum())\n\n# Map diagnosis to class names\ndiagnosis_mapping = {\n    0: 'No_DR', 1: 'Mild', 2: 'Moderate', 3: 'Severe', 4: 'Proliferative_DR'\n}\ndf['class_name'] = df['diagnosis'].map(diagnosis_mapping)\n\n# Plot ORIGINAL class distribution\nplt.figure(figsize=(8,5))\nsns.countplot(x='class_name', data=df, order=list(diagnosis_mapping.values()))\nplt.title(\"Original Distribution of DR Classes\")\nplt.xlabel(\"Diagnosis\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# --- Simple Oversampling Strategy ---\ncounts = df['diagnosis'].value_counts()\nmajority_count = counts[0]\nmax_samples_per_minority = 5000 # Adjust cap if needed\n\ndfs_balanced = [df[df['diagnosis'] == 0]]\n\nfor i in range(1, N_CLASSES):\n    df_minority = df[df['diagnosis'] == i]\n    current_count = len(df_minority)\n    # Increase minority classes up to 5x or the cap, whichever is smaller\n    target_count = min(max_samples_per_minority, current_count * 5)\n    if current_count < target_count:\n        df_minority_oversampled = df_minority.sample(n=target_count, replace=True, random_state=SEED)\n        dfs_balanced.append(df_minority_oversampled)\n        print(f\"Oversampling class {i} ({diagnosis_mapping[i]}) from {current_count} to {target_count}\")\n    else:\n         dfs_balanced.append(df_minority)\n         print(f\"Keeping class {i} ({diagnosis_mapping[i]}) at {current_count} samples (already >= target/cap)\")\n\n\ndf_balanced = pd.concat(dfs_balanced).sample(frac=1, random_state=SEED).reset_index(drop=True) # Shuffle\n\nprint(f\"\\nShape after oversampling minority classes (<= {max_samples_per_minority}): {df_balanced.shape}\")\n\n# Plot BALANCED class distribution\nplt.figure(figsize=(8,5))\nsns.countplot(x='class_name', data=df_balanced, order=list(diagnosis_mapping.values()))\nplt.title(\"Distribution After Oversampling (Minority Cap)\")\nplt.xlabel(\"Diagnosis\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# Use the balanced dataframe from now on\ndf_original = df # Keep original for reference if needed\ndf = df_balanced","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 3: Prepare File Paths and Verify Existence\n# (Keep the version from previous improvements - ensure BASE_DATA_DIR etc. are correct)\n\nBASE_DATA_DIR = '/kaggle/input/diabetic-retinopathy-dataset-ben-graham-applied/DIABETIC_RETINOPATHY_DATASET_BELL_GRAHAM_PROCESSED/BELL_GRAHAM_SEGREGATED'\nactual_folder_mapping = {\n    0: 'No_DR', 1: 'Mild', 2: 'Moderate', 3: 'Severe', 4: 'Proliferative_DR'\n}\nFILE_EXTENSION = \".jpeg\"\n\nprint(\"\\nConstructing image paths...\")\ndf['image_path'] = df.apply(\n    lambda row: os.path.join(\n        BASE_DATA_DIR,\n        actual_folder_mapping.get(row['diagnosis'], 'UNKNOWN_FOLDER'),\n        f\"{row['id_code']}{FILE_EXTENSION}\"\n    ),\n    axis=1\n)\nprint(\"Image paths constructed.\")\n\n# Verify that image files exist (check a sample first for speed)\nprint(\"\\nChecking a sample of image paths...\")\nif df.empty:\n     print(\"DataFrame is empty, skipping path check.\")\nelse:\n    sample_paths = df['image_path'].sample(min(100, len(df)), random_state=SEED)\n    missing_samples = sample_paths[~sample_paths.apply(os.path.exists)]\n    if not missing_samples.empty:\n        print(f\"ERROR: Found missing images!\")\n        print(missing_samples.head())\n        # Decide how to handle: raise error, or remove missing entries\n        # Example: df = df[df['image_path'].apply(os.path.exists)].reset_index(drop=True)\n        #          print(f\"Removed missing files. New df shape: {df.shape}\")\n        raise FileNotFoundError(\"Missing image files found. Please check paths.\")\n    else:\n        print(\"Sample image path check passed.\")\n\n# Create a string version of diagnosis for ImageDataGenerator\ndf['diagnosis_str'] = df['diagnosis'].astype(str)\n\nprint(\"\\nExample constructed paths:\")\nprint(df['image_path'].head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 4: Check Image Readable\n# (Keep the version from previous improvements)\n\ndef check_image_readable(image_path):\n    \"\"\"Checks if an image can be read by OpenCV.\"\"\"\n    img = cv2.imread(image_path)\n    if img is None:\n        print(f\"Warning: Could not read image at {image_path}\")\n        return False\n    return True\n\n# Test reading a sample image\nif not df.empty:\n    sample_path_to_check = df['image_path'].iloc[0]\n    if check_image_readable(sample_path_to_check):\n        print(f\"Successfully read sample image: {sample_path_to_check}\")\n        # Display original image (as read from disk)\n        orig_img = cv2.cvtColor(cv2.imread(sample_path_to_check), cv2.COLOR_BGR2RGB)\n        plt.imshow(orig_img)\n        plt.title(\"Sample Original Image (Read by CV2)\")\n        plt.axis('off')\n        plt.show()\n    else:\n        print(\"Reading failed for the sample image.\")\nelse:\n    print(\"DataFrame is empty, cannot check sample image.\")\n\n# Verification of pixel range (manual check recommended)\n# sample_img_check = cv2.imread(df['image_path'].iloc[0])\n# if sample_img_check is not None:\n#     print(\"Sample image pixel range (min, max):\", np.min(sample_img_check), np.max(sample_img_check))\n#     print(\"Sample image dtype:\", sample_img_check.dtype)\n# else:\n#     print(\"Could not read sample image for pixel range check.\")\n# Confirming uint8 [0, 255] based on user input.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 5: MODIFIED (Data Generators - Use Oversampled Data, Confirmed Rescale)\n\n# Using rescale=1./255 based on uint8 [0, 255] input images.\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.15,\n    height_shift_range=0.15,\n    shear_range=0.15,\n    zoom_range=0.15,\n    horizontal_flip=True,\n    # vertical_flip=False, # Generally not used for retinas\n    brightness_range=[0.8, 1.2],\n    fill_mode='nearest',\n    validation_split=0.2 # Reserve 20% of the oversampled training data for validation\n)\n\ntest_datagen = ImageDataGenerator(rescale=1./255) # Only rescale for test\n\n# Split the OVERSAMPLED data\ntrain_df, test_df = train_test_split(\n    df, # Use the balanced df\n    test_size=0.2,\n    stratify=df['diagnosis_str'], # Stratify on the balanced data\n    random_state=SEED\n)\n\nprint(f\"Train DF shape: {train_df.shape}\")\nprint(f\"Test DF shape: {test_df.shape}\")\n\n# Create generators using the GLOBAL batch size for multi-GPU\ntrain_generator = train_datagen.flow_from_dataframe(\n    train_df,\n    x_col='image_path',\n    y_col='diagnosis_str',\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=global_batch_size,\n    class_mode='categorical',\n    subset='training',\n    shuffle=True,\n    seed=SEED\n)\n\nvalidation_generator = train_datagen.flow_from_dataframe(\n    train_df, # Use the same train_df for validation split\n    x_col='image_path',\n    y_col='diagnosis_str',\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=global_batch_size,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False, # No need to shuffle validation\n    seed=SEED\n)\n\ntest_generator = test_datagen.flow_from_dataframe(\n    test_df,\n    x_col='image_path',\n    y_col='diagnosis_str',\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=global_batch_size,\n    class_mode='categorical',\n    shuffle=False # NEVER shuffle test data\n)\n\n# Class weights (can still be used alongside oversampling/Focal Loss)\nclasses_train = train_df['diagnosis'].values\ncw = class_weight.compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(classes_train),\n    y=classes_train\n)\nclass_weights = {i: cw_val for i, cw_val in enumerate(cw)}\nprint(\"\\nClass weights (calculated on oversampled training data):\")\nprint(class_weights)\n\nprint(f\"\\nFound {train_generator.samples} training samples.\")\nprint(f\"Found {validation_generator.samples} validation samples.\")\nprint(f\"Found {test_generator.samples} test samples.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 6: MODIFIED (Build Model, Compile Phase 1 with AdamW and Focal Loss)\n\ndef build_model(num_classes=N_CLASSES, img_size=IMG_SIZE, fine_tune=False, fine_tune_at_block=4):\n    \"\"\"Builds ResNet50V2 with simplified head. Fine-tuning unfreezes from specified block.\"\"\"\n    input_shape = (img_size, img_size, 3)\n    base_model = applications.ResNet50V2(\n        weights='imagenet', include_top=False, input_shape=input_shape\n    )\n\n    # Determine fine-tuning start layer index\n    fine_tune_layer_name = f'conv{fine_tune_at_block}_block1_out'\n    try:\n        fine_tune_at_index = [i for i, layer in enumerate(base_model.layers) if layer.name == fine_tune_layer_name][0]\n        print(f\"Fine-tuning layer name target: {fine_tune_layer_name} (Index: {fine_tune_at_index})\")\n    except IndexError:\n        print(f\"Warning: Layer '{fine_tune_layer_name}' not found. Using default index 143 for ResNet50V2 Block 4.\")\n        fine_tune_at_index = 143 # Approx start of block 4\n\n    if fine_tune:\n        base_model.trainable = True\n        for layer in base_model.layers[:fine_tune_at_index]:\n            layer.trainable = False\n        num_trainable_base = len(base_model.layers) - fine_tune_at_index\n        print(f\"Fine-tuning enabled: {num_trainable_base} trainable layers in base model (from index {fine_tune_at_index}).\")\n    else:\n        base_model.trainable = False\n        print(\"Fine-tuning disabled: Base model frozen.\")\n\n    # Simplified head\n    model = models.Sequential([\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dropout(0.4),\n        layers.Dense(256, activation='relu'), # Smaller dense layer\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(num_classes, activation='softmax')\n    ])\n    return model\n\n# --- Build and compile Phase 1 model inside strategy scope ---\nif strategy:\n    with strategy.scope():\n        print(\"\\nBuilding and compiling Phase 1 model within MirroredStrategy scope...\")\n        model = build_model(fine_tune=False) # Start with frozen base\n\n        # --- Use AdamW Optimizer ---\n        optimizer_ph1 = tfa.optimizers.AdamW(\n            learning_rate=LR_PHASE1, weight_decay=WD_PHASE1\n        )\n        print(f\"Using AdamW Optimizer for Phase 1: LR={LR_PHASE1}, WD={WD_PHASE1}\")\n\n        # --- Use Categorical Focal Loss ---\n        loss_fn = CategoricalFocalCrossentropy(\n            alpha=FOCAL_ALPHA, gamma=FOCAL_GAMMA\n        )\n        print(f\"Using Categorical Focal Loss: Alpha={FOCAL_ALPHA}, Gamma={FOCAL_GAMMA}\")\n\n        model.compile(\n            optimizer=optimizer_ph1,\n            loss=loss_fn,\n            metrics=['accuracy', tf.keras.metrics.AUC(name='auc')] # Add AUC\n        )\nelse: # Single GPU or CPU\n     print(\"\\nBuilding and compiling Phase 1 model for single device...\")\n     model = build_model(fine_tune=False)\n     optimizer_ph1 = tfa.optimizers.AdamW(learning_rate=LR_PHASE1, weight_decay=WD_PHASE1)\n     loss_fn = CategoricalFocalCrossentropy(alpha=FOCAL_ALPHA, gamma=FOCAL_GAMMA)\n     model.compile(\n         optimizer=optimizer_ph1,\n         loss=loss_fn,\n         metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n     )\n\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 7: MODIFIED (Phase 1 Training - Adjusted Callbacks/Epochs)\n\n# Callbacks for Phase 1\ncheckpoint_path_ph1 = 'model_phase1_best_adamw_focal.keras' # Updated checkpoint name\ncallbacks_phase1 = [\n    EarlyStopping(monitor='val_loss', patience=PATIENCE_EARLY_STOPPING, restore_best_weights=True, verbose=1),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=PATIENCE_REDUCE_LR, min_lr=MIN_LR, verbose=1),\n    ModelCheckpoint(checkpoint_path_ph1, monitor='val_loss', save_best_only=True, verbose=1)\n    # tf.keras.callbacks.TensorBoard(log_dir='./logs_phase1', histogram_freq=1) # Optional TensorBoard\n]\n\nprint(\"\\n--- Starting Phase 1 Training ---\")\nprint(f\"Epochs: {EPOCHS_PHASE1}, Batch Size (Global): {global_batch_size}\")\nprint(f\"Optimizer: AdamW (LR={LR_PHASE1}, WD={WD_PHASE1})\")\nprint(f\"Loss: Categorical Focal Loss (Alpha={FOCAL_ALPHA}, Gamma={FOCAL_GAMMA})\")\nprint(f\"Training samples: {train_generator.samples}, Validation samples: {validation_generator.samples}\")\n\n# Clear session potentially helpful before long training\n# tf.keras.backend.clear_session()\n# gc.collect()\n\nhistory_phase1 = model.fit(\n    train_generator,\n    epochs=EPOCHS_PHASE1,\n    validation_data=validation_generator,\n    callbacks=callbacks_phase1,\n    class_weight=class_weights # Still use weights - can complement Focal Loss/Oversampling\n    # verbose=1 # Default is 1\n)\n\n# Plot training history (Phase 1)\nprint(\"\\n--- Plotting Phase 1 History ---\")\nplt.figure(figsize=(18, 6))\n# ... (plotting code remains the same as previous version) ...\nplt.subplot(1, 3, 1)\nplt.plot(history_phase1.history['loss'], label='Train Loss')\nplt.plot(history_phase1.history['val_loss'], label='Val Loss')\nplt.title(\"Phase 1 Loss\")\nplt.legend()\n\nplt.subplot(1, 3, 2)\nplt.plot(history_phase1.history['accuracy'], label='Train Accuracy')\nplt.plot(history_phase1.history['val_accuracy'], label='Val Accuracy')\nplt.title(\"Phase 1 Accuracy\")\nplt.legend()\n\nplt.subplot(1, 3, 3)\nif 'auc' in history_phase1.history:\n    plt.plot(history_phase1.history['auc'], label='Train AUC')\n    plt.plot(history_phase1.history['val_auc'], label='Val AUC')\n    plt.title(\"Phase 1 AUC\")\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n\nbest_epoch_ph1 = np.argmin(history_phase1.history['val_loss']) + 1\nprint(f\"Best Phase 1 epoch (based on val_loss): {best_epoch_ph1}\")\n\n# Optional cleanup after phase 1\n# del history_phase1\n# gc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 8: MODIFIED (Phase 2 Fine-Tuning with AdamW and Focal Loss)\n\nprint(f\"\\n--- Preparing for Phase 2 Fine-Tuning ---\")\nprint(f\"Loading best weights from Phase 1: {checkpoint_path_ph1}\")\n\n# Load best weights into the *existing* model instance\nmodel.load_weights(checkpoint_path_ph1)\n\n# --- Modify the loaded model for fine-tuning within strategy scope ---\nif strategy:\n    with strategy.scope():\n        print(\"Setting layers trainable and recompiling within strategy scope...\")\n        # Make base model layers trainable\n        base_model = model.layers[0] # Get the base model from the Sequential container\n        base_model.trainable = True\n        fine_tune_at_block = 4 # Match the setting used in build_model definition\n        fine_tune_layer_name = f'conv{fine_tune_at_block}_block1_out'\n        try:\n            fine_tune_at_index = [i for i, layer in enumerate(base_model.layers) if layer.name == fine_tune_layer_name][0]\n        except IndexError:\n            fine_tune_at_index = 143 # Fallback index\n\n        for layer in base_model.layers[:fine_tune_at_index]:\n            layer.trainable = False\n        num_trainable_base = len(base_model.layers) - fine_tune_at_index\n        print(f\"Fine-tuning enabled: {num_trainable_base} trainable layers in base model (from index {fine_tune_at_index}).\")\n\n        # Re-compile with AdamW (lower LR/WD) and Focal Loss\n        optimizer_ph2 = tfa.optimizers.AdamW(\n            learning_rate=LR_PHASE2, weight_decay=WD_PHASE2\n        )\n        print(f\"Using AdamW Optimizer for Phase 2: LR={LR_PHASE2}, WD={WD_PHASE2}\")\n\n        loss_fn = CategoricalFocalCrossentropy( # Use the same loss function instance/params\n             alpha=FOCAL_ALPHA, gamma=FOCAL_GAMMA\n        )\n        print(f\"Using Categorical Focal Loss: Alpha={FOCAL_ALPHA}, Gamma={FOCAL_GAMMA}\")\n\n        model.compile(\n            optimizer=optimizer_ph2,\n            loss=loss_fn,\n            metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n        )\nelse: # Single GPU or CPU\n     print(\"Setting layers trainable and recompiling for single device...\")\n     base_model = model.layers[0]\n     base_model.trainable = True\n     # ... (find fine_tune_at_index logic as above) ...\n     try:\n        fine_tune_at_index = [i for i, layer in enumerate(base_model.layers) if layer.name == f'conv{fine_tune_at_block}_block1_out'][0]\n     except IndexError:\n        fine_tune_at_index = 143\n     for layer in base_model.layers[:fine_tune_at_index]:\n         layer.trainable = False\n     num_trainable_base = len(base_model.layers) - fine_tune_at_index\n     print(f\"Fine-tuning enabled: {num_trainable_base} trainable layers in base model (from index {fine_tune_at_index}).\")\n\n     optimizer_ph2 = tfa.optimizers.AdamW(learning_rate=LR_PHASE2, weight_decay=WD_PHASE2)\n     loss_fn = CategoricalFocalCrossentropy(alpha=FOCAL_ALPHA, gamma=FOCAL_GAMMA)\n     model.compile(\n         optimizer=optimizer_ph2,\n         loss=loss_fn,\n         metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n     )\n\n\nprint(\"Model Summary after enabling fine-tuning:\")\nmodel.summary() # Summary now shows trainable base layers\n\n# Callbacks for Phase 2\ncheckpoint_path_ph2 = 'model_phase2_best_adamw_focal_finetuned.keras' # Updated name\ncallbacks_phase2 = [\n    EarlyStopping(monitor='val_loss', patience=PATIENCE_EARLY_STOPPING, restore_best_weights=True, verbose=1),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=PATIENCE_REDUCE_LR, min_lr=MIN_LR, verbose=1),\n    ModelCheckpoint(checkpoint_path_ph2, monitor='val_loss', save_best_only=True, verbose=1)\n    # tf.keras.callbacks.TensorBoard(log_dir='./logs_phase2', histogram_freq=1) # Optional\n]\n\nprint(\"\\n--- Starting Phase 2 Fine-Tuning ---\")\nprint(f\"Epochs: {EPOCHS_PHASE2}, Batch Size (Global): {global_batch_size}\")\nprint(f\"Optimizer: AdamW (LR={LR_PHASE2}, WD={WD_PHASE2})\")\nprint(f\"Loss: Categorical Focal Loss (Alpha={FOCAL_ALPHA}, Gamma={FOCAL_GAMMA})\")\n\nhistory_phase2 = model.fit(\n    train_generator,\n    epochs=EPOCHS_PHASE2,\n    validation_data=validation_generator,\n    callbacks=callbacks_phase2,\n    class_weight=class_weights, # Continue using class weights if desired\n    initial_epoch=0 # Start fine-tuning epochs from 0\n    # verbose=1\n)\n\n\n# Plot training history (Phase 2)\nprint(\"\\n--- Plotting Phase 2 History ---\")\nplt.figure(figsize=(18, 6))\n# ... (plotting code remains the same as previous version) ...\nplt.subplot(1, 3, 1)\nplt.plot(history_phase2.history['loss'], label='Train Loss')\nplt.plot(history_phase2.history['val_loss'], label='Val Loss')\nplt.title(\"Phase 2 Loss\")\nplt.legend()\n\nplt.subplot(1, 3, 2)\nplt.plot(history_phase2.history['accuracy'], label='Train Accuracy')\nplt.plot(history_phase2.history['val_accuracy'], label='Val Accuracy')\nplt.title(\"Phase 2 Accuracy\")\nplt.legend()\n\nplt.subplot(1, 3, 3)\nif 'auc' in history_phase2.history:\n    plt.plot(history_phase2.history['auc'], label='Train AUC')\n    plt.plot(history_phase2.history['val_auc'], label='Val AUC')\n    plt.title(\"Phase 2 AUC\")\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n\nbest_epoch_ph2 = np.argmin(history_phase2.history['val_loss']) + 1\nprint(f\"Best Phase 2 epoch (based on val_loss): {best_epoch_ph2}\")\n\n# Assign the final model (potentially restored to best weights by EarlyStopping)\nmodel_final = model\n\n# Optional cleanup\n# del history_phase2\n# gc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 9: MODIFIED (Evaluation - Load correct checkpoint, use balanced accuracy)\n\nprint(f\"\\n--- Evaluating Best Fine-Tuned Model from Phase 2 ---\")\n# The 'model_final' variable should hold the best model if EarlyStopping(restore_best_weights=True) worked.\n# Alternatively, explicitly load the best saved checkpoint for certainty.\nprint(f\"Loading model from: {checkpoint_path_ph2}\")\n\nif strategy:\n    with strategy.scope():\n        # Load within scope is often safer\n        model_to_evaluate = tf.keras.models.load_model(checkpoint_path_ph2)\nelse:\n    model_to_evaluate = tf.keras.models.load_model(checkpoint_path_ph2)\n\nprint(\"\\nEvaluating on Test Set...\")\ntest_generator.reset() # Ensure generator starts from the beginning\nresults = model_to_evaluate.evaluate(test_generator, verbose=1)\n\nprint(\"\\nTest Evaluation Results:\")\nprint(f\" - Test Loss: {results[0]:.4f}\")\nprint(f\" - Test Accuracy: {results[1]:.4f}\")\nif len(results) > 2:\n    print(f\" - Test AUC: {results[2]:.4f}\") # Index depends on metrics order\n\nprint(\"\\nGenerating Predictions on Test Set...\")\ntest_generator.reset()\ny_pred_probs = model_to_evaluate.predict(test_generator)\ny_pred_classes = np.argmax(y_pred_probs, axis=1)\ny_true = test_generator.classes\n\n# Map numerical class indices (0, 1, 2...) from generator to actual names\nclass_indices_map = {v: k for k, v in test_generator.class_indices.items()} # {0: '0', 1: '1', ...}\nclass_names_ordered = [diagnosis_mapping[int(class_indices_map[i])] for i in range(N_CLASSES)] # ['No_DR', 'Mild', ...]\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_true, y_pred_classes, target_names=class_names_ordered, digits=3))\n\nbal_acc = balanced_accuracy_score(y_true, y_pred_classes)\nprint(f\"\\nBalanced Test Accuracy: {bal_acc:.4f}\")\n\nprint(\"\\nConfusion Matrix:\")\ncm = confusion_matrix(y_true, y_pred_classes)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=class_names_ordered, yticklabels=class_names_ordered)\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 10: MODIFIED (Single Image Prediction - Use final evaluated model)\n\n# Use the model loaded for evaluation\n# model_to_predict = tf.keras.models.load_model(checkpoint_path_ph2) # Or use this if needed\nmodel_to_predict = model_to_evaluate # Use the model already loaded in Cell 9\n\ndef predict_single_image(image_path, model, img_size=IMG_SIZE):\n    \"\"\"Loads, preprocesses (matching generator), and predicts a single image.\"\"\"\n    try:\n        img = tf.keras.preprocessing.image.load_img(\n            image_path, target_size=(img_size, img_size)\n        )\n        img_array = tf.keras.preprocessing.image.img_to_array(img)\n        # Apply the SAME rescaling used in the generators\n        img_array = img_array / 255.0\n\n        img_batch = np.expand_dims(img_array, axis=0)\n        predictions = model.predict(img_batch)\n        predicted_class_index = np.argmax(predictions[0])\n        confidence = predictions[0][predicted_class_index]\n\n        predicted_class_name = diagnosis_mapping[predicted_class_index]\n        probabilities = {diagnosis_mapping[i]: prob for i, prob in enumerate(predictions[0])}\n\n        return {\n            'predicted_class': predicted_class_name,\n            'confidence': confidence,\n            'probabilities': probabilities,\n            'processed_image_array': img_array\n        }\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {e}\")\n        return None\n\n# Test prediction on a sample test image\n# Ensure test_df is available from Cell 5 split\nif 'test_df' in locals() and not test_df.empty:\n     sample_image_path = test_df['image_path'].iloc[random.randint(0, len(test_df)-1)] # Pick random test image\nelse: # Fallback if test_df not available\n    sample_image_path ='/kaggle/input/diabetic-retinopathy-dataset-ben-graham-applied/DIABETIC_RETINOPATHY_DATASET_BELL_GRAHAM_PROCESSED/BELL_GRAHAM_SEGREGATED/Severe/1008_left.jpeg'\n\nprint(f\"\\n--- Predicting single image: {sample_image_path} ---\")\nresult = predict_single_image(sample_image_path, model_to_predict)\n\nif result:\n    print(f\"Predicted Class: {result['predicted_class']}\")\n    print(f\"Confidence: {result['confidence']:.4f}\")\n    print(\"Class Probabilities:\", result['probabilities'])\n\n    # Display original and the array that went into the model\n    plt.figure(figsize=(12,5))\n    # ... (plotting code remains the same) ...\n    plt.subplot(1,2,1)\n    orig_img = cv2.cvtColor(cv2.imread(sample_image_path), cv2.COLOR_BGR2RGB)\n    plt.imshow(orig_img)\n    plt.title(\"Original Image\")\n    plt.axis('off')\n\n    plt.subplot(1,2,2)\n    plt.imshow(result['processed_image_array'])\n    plt.title(\"Processed Image (as fed to model)\")\n    plt.axis('off')\n    plt.show()\nelse:\n    print(\"Prediction failed.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 11: MODIFIED (Save Final Model - Use the evaluated model)\n\n# Save the final model (the one loaded/evaluated in Cell 9)\nfinal_model_save_path = '/kaggle/working/diabetic_retinopathy_resnet50v2_adamw_focal_final.keras' # Updated name\nprint(f\"\\n--- Saving final model to {final_model_save_path} ---\")\nmodel_to_evaluate.save(final_model_save_path) # Save the model used for final evaluation\nprint(\"Model saved successfully.\")\n\n# Optional: Test loading the final saved model\n# print(\"\\nTesting loading the saved final model...\")\n# loaded_final_model = tf.keras.models.load_model(final_model_save_path)\n# print(\"Model loaded successfully.\")\n\nprint(\"\\n--- Notebook execution finished ---\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}